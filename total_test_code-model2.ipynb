{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8eac8cf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sclab6/anaconda3/envs/test/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "def calc_mean_std(feat, eps=1e-5):\n",
    "    # eps is a small value added to the variance to avoid divide-by-zero.\n",
    "    size = feat.size()\n",
    "    assert (len(size) == 4)\n",
    "    N, C = size[:2]\n",
    "    feat_var = feat.view(N, C, -1).var(dim=2) + eps\n",
    "    feat_std = feat_var.sqrt().view(N, C, 1, 1)\n",
    "    feat_mean = feat.view(N, C, -1).mean(dim=2).view(N, C, 1, 1)\n",
    "    return feat_mean, feat_std\n",
    "\n",
    "\n",
    "def adaptive_instance_normalization(content_feat, style_feat):\n",
    "    assert (content_feat.size()[:2] == style_feat.size()[:2])\n",
    "    size = content_feat.size()\n",
    "    style_mean, style_std = calc_mean_std(style_feat)\n",
    "    content_mean, content_std = calc_mean_std(content_feat)\n",
    "\n",
    "    normalized_feat = (content_feat - content_mean.expand(\n",
    "        size)) / content_std.expand(size)\n",
    "    return normalized_feat * style_std.expand(size) + style_mean.expand(size)\n",
    "\n",
    "\n",
    "def _calc_feat_flatten_mean_std(feat):\n",
    "    # takes 3D feat (C, H, W), return mean and std of array within channels\n",
    "    assert (feat.size()[0] == 3)\n",
    "    assert (isinstance(feat, torch.FloatTensor))\n",
    "    feat_flatten = feat.view(3, -1)\n",
    "    mean = feat_flatten.mean(dim=-1, keepdim=True)\n",
    "    std = feat_flatten.std(dim=-1, keepdim=True)\n",
    "    return feat_flatten, mean, std\n",
    "\n",
    "\n",
    "def _mat_sqrt(x):\n",
    "    U, D, V = torch.svd(x)\n",
    "    return torch.mm(torch.mm(U, D.pow(0.5).diag()), V.t())\n",
    "\n",
    "\n",
    "def coral(source, target):\n",
    "    # assume both source and target are 3D array (C, H, W)\n",
    "    # Note: flatten -> f\n",
    "\n",
    "    source_f, source_f_mean, source_f_std = _calc_feat_flatten_mean_std(source)\n",
    "    source_f_norm = (source_f - source_f_mean.expand_as(\n",
    "        source_f)) / source_f_std.expand_as(source_f)\n",
    "    source_f_cov_eye = \\\n",
    "        torch.mm(source_f_norm, source_f_norm.t()) + torch.eye(3)\n",
    "\n",
    "    target_f, target_f_mean, target_f_std = _calc_feat_flatten_mean_std(target)\n",
    "    target_f_norm = (target_f - target_f_mean.expand_as(\n",
    "        target_f)) / target_f_std.expand_as(target_f)\n",
    "    target_f_cov_eye = \\\n",
    "        torch.mm(target_f_norm, target_f_norm.t()) + torch.eye(3)\n",
    "\n",
    "    source_f_norm_transfer = torch.mm(\n",
    "        _mat_sqrt(target_f_cov_eye),\n",
    "        torch.mm(torch.inverse(_mat_sqrt(source_f_cov_eye)),\n",
    "                 source_f_norm)\n",
    "    )\n",
    "\n",
    "    source_f_transfer = source_f_norm_transfer * \\\n",
    "                        target_f_std.expand_as(source_f_norm) + \\\n",
    "                        target_f_mean.expand_as(source_f_norm)\n",
    "\n",
    "    return source_f_transfer.view(source.size())\n",
    "\n",
    "def rgb_to_yiq(img): # img shape - batch size channel width height\n",
    "    bsz, ch, w, h = img.shape\n",
    "    yiq_from_rgb = torch.Tensor([[0.299,      0.587,        0.114],\n",
    "                                 [0.59590059, -0.27455667, -0.32134392],\n",
    "                                 [0.21153661, -0.52273617, 0.31119955]]).to(img.device)\n",
    "\n",
    "    out = img.permute(1,0,2,3).reshape(ch, -1)\n",
    "    out = torch.matmul(yiq_from_rgb, out)\n",
    "    out = out.reshape(ch, bsz, w, h).permute(1,0,2,3)\n",
    "    return out\n",
    "\n",
    "def yiq_to_rgb(img):\n",
    "    bsz, ch, w, h = img.shape\n",
    "    yiq_from_rgb = torch.Tensor([[0.299,      0.587,        0.114],\n",
    "                                 [0.59590059, -0.27455667, -0.32134392],\n",
    "                                 [0.21153661, -0.52273617, 0.31119955]]).to(img.device)\n",
    "    rgb_from_yiq = torch.inverse(yiq_from_rgb)\n",
    "    out = img.permute(1,0,2,3).reshape(ch, -1)\n",
    "    out = torch.matmul(rgb_from_yiq, out)\n",
    "    out = out.reshape(ch, bsz, w, h).permute(1,0,2,3)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "db39365a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "# from function import adaptive_instance_normalization as adain\n",
    "# from function import calc_mean_std\n",
    "\n",
    "decoder1 = nn.Sequential(\n",
    "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
    "    nn.Conv2d(512, 256, (3, 3)),\n",
    "    nn.ReLU(),\n",
    "    nn.Upsample(scale_factor=2, mode='nearest'),\n",
    "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
    "    nn.Conv2d(256, 256, (3, 3)),\n",
    "    nn.ReLU(),\n",
    "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
    "    nn.Conv2d(256, 256, (3, 3)),\n",
    "    nn.ReLU(),\n",
    "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
    "    nn.Conv2d(256, 256, (3, 3)),\n",
    "    nn.ReLU(),\n",
    "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
    "    nn.Conv2d(256, 128, (3, 3)),\n",
    "    nn.ReLU(),\n",
    "    nn.Upsample(scale_factor=2, mode='nearest'),\n",
    "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
    "    nn.Conv2d(128, 128, (3, 3)),\n",
    "    nn.ReLU(),\n",
    "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
    "    nn.Conv2d(128, 64, (3, 3)),\n",
    "    nn.ReLU(),\n",
    "    nn.Upsample(scale_factor=2, mode='nearest'),\n",
    "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
    "    nn.Conv2d(64, 64, (3, 3)),\n",
    "    nn.ReLU(),\n",
    "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
    "    nn.Conv2d(64, 1, (3, 3)),\n",
    ")\n",
    "\n",
    "decoder2 = nn.Sequential(\n",
    "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
    "    nn.Conv2d(512, 256, (3, 3)),\n",
    "    nn.ReLU(),\n",
    "    nn.Upsample(scale_factor=2, mode='nearest'),\n",
    "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
    "    nn.Conv2d(256, 256, (3, 3)),\n",
    "    nn.ReLU(),\n",
    "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
    "    nn.Conv2d(256, 256, (3, 3)),\n",
    "    nn.ReLU(),\n",
    "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
    "    nn.Conv2d(256, 256, (3, 3)),\n",
    "    nn.ReLU(),\n",
    "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
    "    nn.Conv2d(256, 128, (3, 3)),\n",
    "    nn.ReLU(),\n",
    "    nn.Upsample(scale_factor=2, mode='nearest'),\n",
    "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
    "    nn.Conv2d(128, 128, (3, 3)),\n",
    "    nn.ReLU(),\n",
    "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
    "    nn.Conv2d(128, 64, (3, 3)),\n",
    "    nn.ReLU(),\n",
    "    nn.Upsample(scale_factor=2, mode='nearest'),\n",
    "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
    "    nn.Conv2d(64, 64, (3, 3)),\n",
    "    nn.ReLU(),\n",
    "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
    "    nn.Conv2d(64, 2, (3, 3)),\n",
    ")\n",
    "\n",
    "vgg = nn.Sequential(\n",
    "    nn.Conv2d(3, 3, (1, 1)),\n",
    "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
    "    nn.Conv2d(3, 64, (3, 3)),\n",
    "    nn.ReLU(),  # relu1-1\n",
    "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
    "    nn.Conv2d(64, 64, (3, 3)),\n",
    "    nn.ReLU(),  # relu1-2\n",
    "    nn.MaxPool2d((2, 2), (2, 2), (0, 0), ceil_mode=True),\n",
    "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
    "    nn.Conv2d(64, 128, (3, 3)),\n",
    "    nn.ReLU(),  # relu2-1\n",
    "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
    "    nn.Conv2d(128, 128, (3, 3)),\n",
    "    nn.ReLU(),  # relu2-2\n",
    "    nn.MaxPool2d((2, 2), (2, 2), (0, 0), ceil_mode=True),\n",
    "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
    "    nn.Conv2d(128, 256, (3, 3)),\n",
    "    nn.ReLU(),  # relu3-1\n",
    "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
    "    nn.Conv2d(256, 256, (3, 3)),\n",
    "    nn.ReLU(),  # relu3-2\n",
    "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
    "    nn.Conv2d(256, 256, (3, 3)),\n",
    "    nn.ReLU(),  # relu3-3\n",
    "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
    "    nn.Conv2d(256, 256, (3, 3)),\n",
    "    nn.ReLU(),  # relu3-4\n",
    "    nn.MaxPool2d((2, 2), (2, 2), (0, 0), ceil_mode=True),\n",
    "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
    "    nn.Conv2d(256, 512, (3, 3)),\n",
    "    nn.ReLU(),  # relu4-1, this is the last layer used\n",
    "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
    "    nn.Conv2d(512, 512, (3, 3)),\n",
    "    nn.ReLU(),  # relu4-2\n",
    "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
    "    nn.Conv2d(512, 512, (3, 3)),\n",
    "    nn.ReLU(),  # relu4-3\n",
    "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
    "    nn.Conv2d(512, 512, (3, 3)),\n",
    "    nn.ReLU(),  # relu4-4\n",
    "    nn.MaxPool2d((2, 2), (2, 2), (0, 0), ceil_mode=True),\n",
    "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
    "    nn.Conv2d(512, 512, (3, 3)),\n",
    "    nn.ReLU(),  # relu5-1\n",
    "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
    "    nn.Conv2d(512, 512, (3, 3)),\n",
    "    nn.ReLU(),  # relu5-2\n",
    "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
    "    nn.Conv2d(512, 512, (3, 3)),\n",
    "    nn.ReLU(),  # relu5-3\n",
    "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
    "    nn.Conv2d(512, 512, (3, 3)),\n",
    "    nn.ReLU()  # relu5-4\n",
    ")\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(Net, self).__init__()\n",
    "        enc_layers = list(encoder.children())\n",
    "        self.enc_1 = nn.Sequential(*enc_layers[:4])  # input -> relu1_1\n",
    "        self.enc_2 = nn.Sequential(*enc_layers[4:11])  # relu1_1 -> relu2_1\n",
    "        self.enc_3 = nn.Sequential(*enc_layers[11:18])  # relu2_1 -> relu3_1\n",
    "        self.enc_4 = nn.Sequential(*enc_layers[18:31])  # relu3_1 -> relu4_1\n",
    "        self.decoder = decoder\n",
    "        self.mse_loss = nn.MSELoss()\n",
    "\n",
    "        # fix the encoder\n",
    "        for name in ['enc_1', 'enc_2', 'enc_3', 'enc_4']:\n",
    "            for param in getattr(self, name).parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "    # extract relu1_1, relu2_1, relu3_1, relu4_1 from input image\n",
    "    def encode_with_intermediate(self, input):\n",
    "        results = [input]\n",
    "        for i in range(4):\n",
    "            func = getattr(self, 'enc_{:d}'.format(i + 1))\n",
    "            results.append(func(results[-1]))\n",
    "        return results[1:]\n",
    "\n",
    "    # extract relu4_1 from input image\n",
    "    def encode(self, input):\n",
    "        for i in range(4):\n",
    "            input = getattr(self, 'enc_{:d}'.format(i + 1))(input)\n",
    "        return input\n",
    "\n",
    "    def calc_content_loss(self, input, target):\n",
    "        assert (input.size() == target.size())\n",
    "        assert (target.requires_grad is False)\n",
    "        return self.mse_loss(input, target)\n",
    "\n",
    "    def calc_style_loss(self, input, target):\n",
    "        assert (input.size() == target.size())\n",
    "        assert (target.requires_grad is False)\n",
    "        input_mean, input_std = calc_mean_std(input)\n",
    "        target_mean, target_std = calc_mean_std(target)\n",
    "        return self.mse_loss(input_mean, target_mean) + \\\n",
    "               self.mse_loss(input_std, target_std)\n",
    "\n",
    "    def forward(self, content, style, alpha=1.0):\n",
    "        assert 0 <= alpha <= 1\n",
    "        \n",
    "        # convert to yiq\n",
    "        # 방법1 - encoder에 y 채널만 넣어서 feature 추출하여서 그걸로 y channel에 해당하는 이미지 생성\n",
    "        # 방법2 - encoder에 rgb 이미지 그대로 넣고 decoder에서 y channel만 생성\n",
    "        \n",
    "        content_yiq = rgb_to_yiq(content)\n",
    "        style_yiq = rgb_to_yiq(style)\n",
    "        \n",
    "        content_y = torch.cat((content_yiq[:,0],content_yiq[:,0],content_yiq[:,0]), dim=1)\n",
    "        style_y = torch.cat((style_yiq[:,0],style_yiq[:,0],style_yiq[:,0]), dim=1)\n",
    "        \n",
    "        style_feats = self.encode_with_intermediate(style_y)\n",
    "        content_feat = self.encode(content_y)\n",
    "        t = adaptive_instance_normalization(content_feat, style_feats[-1])\n",
    "        t = alpha * t + (1 - alpha) * content_feat\n",
    "\n",
    "        g_t = self.decoder(t)\n",
    "        # g_t : y channel\n",
    "        g_t = torch.cat((g_t, content_yiq[:,1:]),dim=1)\n",
    "        g_t = yiq_to_rgb(g_t)\n",
    "        \n",
    "        g_t_feats = self.encode_with_intermediate(g_t)\n",
    "\n",
    "        loss_c = self.calc_content_loss(g_t_feats[-1], t)\n",
    "        loss_s = self.calc_style_loss(g_t_feats[0], style_feats[0])\n",
    "        for i in range(1, 4):\n",
    "            loss_s += self.calc_style_loss(g_t_feats[i], style_feats[i])\n",
    "        return loss_c, loss_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a8a02b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torch.utils import data\n",
    "\n",
    "def InfiniteSampler(n):\n",
    "    # i = 0\n",
    "    i = n - 1\n",
    "    order = np.random.permutation(n)\n",
    "    while True:\n",
    "        yield order[i]\n",
    "        i += 1\n",
    "        if i >= n:\n",
    "            np.random.seed()\n",
    "            order = np.random.permutation(n)\n",
    "            i = 0\n",
    "\n",
    "class InfiniteSamplerWrapper(data.sampler.Sampler):\n",
    "    def __init__(self, data_source):\n",
    "        self.num_samples = len(data_source)\n",
    "\n",
    "    def __iter__(self):\n",
    "        return iter(InfiniteSampler(self.num_samples))\n",
    "\n",
    "    def __len__(self):\n",
    "        return 2 ** 31"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a5fdbaf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "from math import exp\n",
    "\n",
    "def gaussian(window_size, sigma):\n",
    "    gauss = torch.Tensor([exp(-(x - window_size//2)**2/float(2*sigma**2)) for x in range(window_size)])\n",
    "    return gauss/gauss.sum()\n",
    "\n",
    "def create_window(window_size, channel):\n",
    "    _1D_window = gaussian(window_size, 1.5).unsqueeze(1)\n",
    "    _2D_window = _1D_window.mm(_1D_window.t()).float().unsqueeze(0).unsqueeze(0)\n",
    "    window = Variable(_2D_window.expand(channel, 1, window_size, window_size).contiguous())\n",
    "    return window\n",
    "\n",
    "def _ssim(img1, img2, window, window_size, channel, size_average = True):\n",
    "    mu1 = F.conv2d(img1, window, padding = window_size//2, groups = channel)\n",
    "    mu2 = F.conv2d(img2, window, padding = window_size//2, groups = channel)\n",
    "\n",
    "    mu1_sq = mu1.pow(2)\n",
    "    mu2_sq = mu2.pow(2)\n",
    "    mu1_mu2 = mu1*mu2\n",
    "\n",
    "    sigma1_sq = F.conv2d(img1*img1, window, padding = window_size//2, groups = channel) - mu1_sq\n",
    "    sigma2_sq = F.conv2d(img2*img2, window, padding = window_size//2, groups = channel) - mu2_sq\n",
    "    sigma12 = F.conv2d(img1*img2, window, padding = window_size//2, groups = channel) - mu1_mu2\n",
    "\n",
    "    C1 = 0.01**2\n",
    "    C2 = 0.03**2\n",
    "\n",
    "    ssim_map = ((2*mu1_mu2 + C1)*(2*sigma12 + C2))/((mu1_sq + mu2_sq + C1)*(sigma1_sq + sigma2_sq + C2))\n",
    "\n",
    "    if size_average:\n",
    "        return ssim_map.mean()\n",
    "    else:\n",
    "        return ssim_map.mean(1).mean(1).mean(1)\n",
    "\n",
    "class SSIM(torch.nn.Module):\n",
    "    def __init__(self, window_size = 11, size_average = True):\n",
    "        super(SSIM, self).__init__()\n",
    "        self.window_size = window_size\n",
    "        self.size_average = size_average\n",
    "        self.channel = 1\n",
    "        self.window = create_window(window_size, self.channel)\n",
    "\n",
    "    def forward(self, img1, img2):\n",
    "        (_, channel, _, _) = img1.size()\n",
    "\n",
    "        if channel == self.channel and self.window.data.type() == img1.data.type():\n",
    "            window = self.window\n",
    "        else:\n",
    "            window = create_window(self.window_size, channel)\n",
    "            \n",
    "            if img1.is_cuda:\n",
    "                window = window.cuda(img1.get_device())\n",
    "            window = window.type_as(img1)\n",
    "            \n",
    "            self.window = window\n",
    "            self.channel = channel\n",
    "\n",
    "\n",
    "        return _ssim(img1, img2, window, self.window_size, channel, self.size_average)\n",
    "\n",
    "def ssim(img1, img2, window_size = 11, size_average = True):\n",
    "    (_, channel, _, _) = img1.size()\n",
    "    window = create_window(window_size, channel)\n",
    "    \n",
    "    if img1.is_cuda:\n",
    "        window = window.cuda(img1.get_device())\n",
    "    window = window.type_as(img1)\n",
    "    \n",
    "    return _ssim(img1, img2, window, window_size, channel, size_average)\n",
    "\n",
    "def calc_mean_std(feat, eps=1e-5):\n",
    "    # eps is a small value added to the variance to avoid divide-by-zero.\n",
    "    size = feat.size()\n",
    "    assert (len(size) == 4)\n",
    "    N, C = size[:2]\n",
    "    feat_var = feat.view(N, C, -1).var(dim=2) + eps\n",
    "    feat_std = feat_var.sqrt().view(N, C, 1, 1)\n",
    "    feat_mean = feat.view(N, C, -1).mean(dim=2).view(N, C, 1, 1)\n",
    "    return feat_mean, feat_std\n",
    "\n",
    "\n",
    "\n",
    "def calc_style_loss(input, target):\n",
    "    assert (input.size() == target.size())\n",
    "    #assert (target.requires_grad is False)\n",
    "    input_mean, input_std = calc_mean_std(input)\n",
    "    target_mean, target_std = calc_mean_std(target)\n",
    "    return nn.MSELoss()(input_mean, target_mean) + \\\n",
    "           nn.MSELoss()(input_std, target_std)\n",
    "\n",
    "def style_loss(img1, img2, vgg):\n",
    "    enc_layers = list(vgg.children())\n",
    "    enc_1 = nn.Sequential(*enc_layers[:4])  # input -> relu1_1\n",
    "    enc_2 = nn.Sequential(*enc_layers[4:11])  # relu1_1 -> relu2_1\n",
    "    enc_3 = nn.Sequential(*enc_layers[11:18])  # relu2_1 -> relu3_1\n",
    "    enc_4 = nn.Sequential(*enc_layers[18:31])  # relu3_1 -> relu4_1\n",
    "    \n",
    "    results1 = []\n",
    "    results1.append(enc_1(img1))\n",
    "    results1.append(enc_2(results1[0]))\n",
    "    results1.append(enc_3(results1[1]))\n",
    "    results1.append(enc_4(results1[2]))\n",
    "    \n",
    "    results2 = []\n",
    "    results2.append(enc_1(img2))\n",
    "    results2.append(enc_2(results2[0]))\n",
    "    results2.append(enc_3(results2[1]))\n",
    "    results2.append(enc_4(results2[2]))\n",
    "    \n",
    "    loss=0\n",
    "    for i in range(4):\n",
    "        loss += calc_style_loss(results1[i], results2[i])\n",
    "    return loss\n",
    "\n",
    "def color_loss(img1, img2):\n",
    "    his_img1r = torch.histc(img1[0][0], bins=256, min=0, max=0)\n",
    "    his_img2r = torch.histc(img2[0][0], bins=256, min=0, max=0)\n",
    "    his_img1g = torch.histc(img1[0][1], bins=256, min=0, max=0)\n",
    "    his_img2g = torch.histc(img2[0][1], bins=256, min=0, max=0)\n",
    "    his_img1b = torch.histc(img1[0][2], bins=256, min=0, max=0)\n",
    "    his_img2b = torch.histc(img2[0][2], bins=256, min=0, max=0)\n",
    "    \n",
    "    l1_r = torch.mean(torch.abs(his_img1r) - torch.abs(his_img2r))\n",
    "    l1_g = torch.mean(torch.abs(his_img1g) - torch.abs(his_img2g))\n",
    "    l1_b = torch.mean(torch.abs(his_img1b) - torch.abs(his_img2b))\n",
    "    l1_ = (l1_r+l1_g+l1_b) / 3\n",
    "    \n",
    "    m_r = his_img1r.mean() - his_img2r.mean()\n",
    "    m_g = his_img1g.mean() - his_img2g.mean()\n",
    "    m_b = his_img1b.mean() - his_img2b.mean()\n",
    "    m_ = (m_r+m_g+m_b)/3\n",
    "    \n",
    "    s_r = his_img1r.std() - his_img2r.std()\n",
    "    s_g = his_img1g.std() - his_img2g.std()\n",
    "    s_b = his_img1b.std() - his_img2b.std()\n",
    "    s_ = (s_r+s_g+s_b)/3\n",
    "    \n",
    "    return l1_ + m_ + s_ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa3b7eb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n",
      "5000\n",
      "16\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "from pathlib import Path\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import save_image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#import net\n",
    "#from function import adaptive_instance_normalization, coral\n",
    "\n",
    "def test_transform(size, crop):\n",
    "    transform_list = []\n",
    "    if size != 0:\n",
    "        transform_list.append(transforms.Resize(size=(size,size)))\n",
    "    if crop:\n",
    "        transform_list.append(transforms.CenterCrop(size))\n",
    "    transform_list.append(transforms.ToTensor())\n",
    "    transform = transforms.Compose(transform_list)\n",
    "    return transform\n",
    "\n",
    "def style_transfer(vgg, decoder_y, decoder_iq, content, style, color, alpha=1.0,\n",
    "                   interpolation_weights=None):\n",
    "    assert (0.0 <= alpha <= 1.0)\n",
    "    content_yiq = rgb_to_yiq(content)\n",
    "    color_yiq = rgb_to_yiq(color)\n",
    "    style_yiq = rgb_to_yiq(style)\n",
    "\n",
    "    content_y = torch.cat((content_yiq[:,0].unsqueeze(1),content_yiq[:,0].unsqueeze(1),content_yiq[:,0].unsqueeze(1)),dim=1)\n",
    "    style_y = torch.cat((style_yiq[:,0].unsqueeze(1),style_yiq[:,0].unsqueeze(1),style_yiq[:,0].unsqueeze(1)),dim=1)\n",
    "    content_with_color = torch.cat((content_yiq[:,0].unsqueeze(1),color_yiq[:,1:]),dim=1)\n",
    "\n",
    "    content_f = vgg(content_y)\n",
    "    style_f = vgg(style_y)\n",
    "    color_f = vgg(content_with_color)\n",
    "\n",
    "    tt = adaptive_instance_normalization(content_f, style_f)\n",
    "    tt = alpha * tt + (1 - alpha) * content_f\n",
    "\n",
    "    tc = adaptive_instance_normalization(content_f, color_f)\n",
    "    tc = alpha * tc + (1 - alpha) * content_f\n",
    "    output_y = decoder_y(tt)\n",
    "    output_iq = decoder_iq(tc)\n",
    "    output = torch.cat((output_y, output_iq), dim=1)\n",
    "    output = yiq_to_rgb(output)\n",
    "    return output\n",
    "\n",
    "\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "# Basic options\n",
    "parser.add_argument('--content' , type=str, #default='./test/input/content//brad_pitt.jpg',\n",
    "                    help='File path to the content image')\n",
    "parser.add_argument('--content_dir', type=str, default='./val2017',\n",
    "                    help='Directory path to a batch of content images')\n",
    "parser.add_argument('--style', type=str, #default='./test/input/style/en_campo_gris.jpg',\n",
    "                    help='File path to the style image, or multiple style \\\n",
    "                    images separated by commas if you want to do style \\\n",
    "                    interpolation or spatial control')\n",
    "parser.add_argument('--style_dir', type=str, default='./data/test',\n",
    "                    help='Directory path to a batch of style images')\n",
    "parser.add_argument('--color', type=str, #default='./test/input/color/cyberpunk_city.jpg',\n",
    "                    help='File path to the style image, or multiple style \\\n",
    "                    images separated by commas if you want to do style \\\n",
    "                    interpolation or spatial control')\n",
    "parser.add_argument('--color_dir', type=str, default='./target_images',\n",
    "                    help='Directory path to a batch of style images')\n",
    "parser.add_argument('--vgg', type=str, default='models/vgg_normalised.pth')\n",
    "parser.add_argument('--decoder', type=str, default='experiments_model2/decoder_iter_80000.pth.tar')\n",
    "\n",
    "# Additional options\n",
    "parser.add_argument('--content_size', type=int, default=512,\n",
    "                    help='New (minimum) size for the content image, \\\n",
    "                    keeping the original size if set to 0')\n",
    "parser.add_argument('--style_size', type=int, default=512,\n",
    "                    help='New (minimum) size for the style image, \\\n",
    "                    keeping the original size if set to 0')\n",
    "parser.add_argument('--crop', action='store_true',\n",
    "                    help='do center crop to create squared image')\n",
    "parser.add_argument('--save_ext', default='.jpg',\n",
    "                    help='The extension name of the output image')\n",
    "parser.add_argument('--output', type=str, default='output/model2',\n",
    "                    help='Directory to save the output image(s)')\n",
    "\n",
    "# Advanced options\n",
    "parser.add_argument('--preserve_color', action='store_true',\n",
    "                    help='If specified, preserve color of the content image')\n",
    "parser.add_argument('--alpha', type=float, default=1.0,\n",
    "                    help='The weight that controls the degree of \\\n",
    "                             stylization. Should be between 0 and 1')\n",
    "parser.add_argument(\n",
    "    '--style_interpolation_weights', type=str, default='',\n",
    "    help='The weight for blending the style of multiple style images')\n",
    "\n",
    "args = parser.parse_args([])\n",
    "\n",
    "do_interpolation = False\n",
    "\n",
    "device = torch.device(\"cuda:3\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "output_dir = Path(args.output)\n",
    "output_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "# Either --content or --contentDir should be given.\n",
    "assert (args.content or args.content_dir)\n",
    "if args.content:\n",
    "    content_paths = [Path(args.content)]\n",
    "else:\n",
    "    content_dir = Path(args.content_dir)\n",
    "    content_paths = [f for f in content_dir.glob('*')]\n",
    "\n",
    "# Either --style or --styleDir should be given.\n",
    "assert (args.style or args.style_dir)\n",
    "if args.style:\n",
    "    style_paths = args.style.split(',')\n",
    "    if len(style_paths) == 1:\n",
    "        style_paths = [Path(args.style)]\n",
    "    else:\n",
    "        do_interpolation = True\n",
    "        assert (args.style_interpolation_weights != ''), \\\n",
    "            'Please specify interpolation weights'\n",
    "        weights = [int(i) for i in args.style_interpolation_weights.split(',')]\n",
    "        interpolation_weights = [w / sum(weights) for w in weights]\n",
    "else:\n",
    "    style_dir = Path(args.style_dir)\n",
    "    style_paths = [f for f in style_dir.glob('*')]\n",
    "\n",
    "assert (args.color or args.color_dir)\n",
    "if args.color:\n",
    "    color_paths = [Path(args.color)]\n",
    "else:\n",
    "    color_dir = Path(args.color_dir)\n",
    "    color_paths = [f for f in color_dir.glob('*')]\n",
    "    \n",
    "decoder_y = decoder1\n",
    "decoder_iq = decoder2\n",
    "vgg = vgg\n",
    "\n",
    "decoder_y.eval()\n",
    "decoder_iq.eval()\n",
    "vgg.eval()\n",
    "\n",
    "decoder_y.load_state_dict(torch.load(args.decoder)['decoder_y'])\n",
    "decoder_iq.load_state_dict(torch.load(args.decoder)['decoder_iq'])\n",
    "vgg.load_state_dict(torch.load(args.vgg))\n",
    "vgg = nn.Sequential(*list(vgg.children())[:31])\n",
    "\n",
    "vgg.to(device)\n",
    "decoder_y.to(device)\n",
    "decoder_iq.to(device)\n",
    "\n",
    "content_tf = test_transform(args.content_size, args.crop)\n",
    "style_tf = test_transform(args.style_size, args.crop)\n",
    "color_tf = test_transform(args.style_size, args.crop)\n",
    "            \n",
    "total_SSIM = 0\n",
    "total_styleloss = 0\n",
    "total_colorloss = 0\n",
    "\n",
    "print(len(content_paths))\n",
    "print(len(style_paths))\n",
    "color_num = len(color_paths)\n",
    "print(len(color_paths))\n",
    "total_num = len(content_paths)*len(style_paths)*len(color_paths)\n",
    "\n",
    "random.shuffle(content_paths)\n",
    "random.shuffle(style_paths)\n",
    "random.shuffle(color_paths)\n",
    "for i in range(len(content_paths)):\n",
    "    content_path = content_paths[i]\n",
    "    style_path = style_paths[i]\n",
    "    color_path = color_paths[i % color_num]\n",
    "    \n",
    "    content = content_tf(Image.open(str(content_path)).convert(\"RGB\"))\n",
    "    style = style_tf(Image.open(str(style_path)).convert(\"RGB\"))\n",
    "    color = color_tf(Image.open(str(color_path)).convert(\"RGB\"))\n",
    "    if content.shape[0] == 1:\n",
    "        content = torch.cat((content, content, content),dim=0)\n",
    "    if style.shape[0] == 1:\n",
    "        style = torch.cat((style,style,style),dim=0)\n",
    "    if color.shape[0] == 1:\n",
    "        color = torch.cat((color,color,color),dim=0)\n",
    "        \n",
    "    content = content.to(device).unsqueeze(0)\n",
    "    style = style.to(device).unsqueeze(0)\n",
    "    color = color.to(device).unsqueeze(0)\n",
    "    with torch.no_grad():\n",
    "        output = style_transfer(vgg, decoder_y, decoder_iq, content, style, color,\n",
    "                                            args.alpha)\n",
    "\n",
    "        total_SSIM += ssim(content, output).item()\n",
    "        total_styleloss += style_loss(style, output, vgg)\n",
    "        total_colorloss += color_loss(color, output)\n",
    "        output = output.cpu()\n",
    "\n",
    "#         output_name = output_dir / '{:s}_stylized_{:s}{:s}{:s}'.format(\n",
    "#             content_path.stem, style_path.stem, color_path.stem, args.save_ext)\n",
    "\n",
    "#         fig = plt.figure(figsize=(20, 8))\n",
    "#         ax = fig.add_subplot(1, 4, 1)\n",
    "#         #imgplot = plt.imshow(Image.open(str(content_path)))\n",
    "#         imgplot = plt.imshow(content.squeeze(0).permute(1,2,0).cpu().numpy())\n",
    "#         ax.set_title('content')\n",
    "#         ax = fig.add_subplot(1, 4, 2)\n",
    "#         #imgplot = plt.imshow(Image.open(str(style_path)))\n",
    "#         imgplot = plt.imshow(style.squeeze(0).permute(1,2,0).cpu().numpy())\n",
    "#         ax.set_title('texture')\n",
    "#         ax = fig.add_subplot(1, 4, 3)\n",
    "#         #imgplot = plt.imshow(Image.open(str(color_path)))\n",
    "#         imgplot = plt.imshow(color.squeeze(0).permute(1,2,0).cpu().numpy())\n",
    "#         ax.set_title('color')\n",
    "#         ax = fig.add_subplot(1, 4, 4)\n",
    "#         imgplot = plt.imshow(output.squeeze(0).permute(1,2,0).numpy())\n",
    "#         ax.set_title('output')\n",
    "\n",
    "#         plt.savefig(str(output_name))\n",
    "\n",
    "        #save_image(output, str(output_name))\n",
    "\n",
    "print(total_SSIM/5000)\n",
    "print(total_styleloss / 5000)\n",
    "print(total_colorloss / 5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "170ca4dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
